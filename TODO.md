# TODO:
## Immediate:
- [x] Finish adjusting diagram to be a detailed overview
- [x] Add load/store benchmarks
- [x] Add load/store saturation
- [ ] adjust build chain before adding NAS parallel benchmarks
  - [x] merge polynomial_expansion into this repo, remove submodule
  - [x] Empty makefile to start over. 
  - [x] rename *.h to *.hpp
  - [x] remove compile commands in convolution/makefile that are not reused
  - [x] simplify config.mk
  - [x] test makefile
  - [x] change how fhv and others are run (LD_LIBRARY_PATH now must include
        path to libfhv_perfmon.so)
  - [x] simplify polynomial_expansion/makefile
  - [x] Build "performance_monitor.cpp" into a shared library
- [ ] Setup NAS parallel benchmarks
  - [ ] for now, keep as separate project (we don't depend on anything from
        here except built library, anyways). These can be added to examples
        later if we can simplify the code.
  - [ ] fork repo, adjust it to use fhv performance monitor
- [ ] Using these tests, compare our application to intel vTune
- [ ] explore how well fhv works with other kernels and codebases
  - [ ] consider NAS parallel benchmarks (see above)
  - [ ] Dr. Saule may be able to throw together some software that
        demonstrates stress on more granular things like TLB or instruction
        decoder
- [ ] start exploring different machines
  - [ ] another skylake architecture with a different number of cores
  - [ ] broadwell/haswell
  - [ ] eventual goal is to have architecture detection totally automatic but
        for now it's adequate to have a few sets of parameters hardcoded that
        are selected automatically

## Long-term:
### Problems to fix:
- fix benchmark-likwid-vs-manual and thread_migration 
- manual benchmark only prints runtime for flops region
  - in other words, runtime_by_tag doesn't seem to work for more than one 
    region
- sometimes make rule for `run-tests/fhv_minimal` fails with a segmentation
  fault, seems to be right after compilation but before running. Immediately
  running the rule again succeeds.
- sometimes get "stopping non-started region" error in fhv. I think it only
  happens when you run "benchmark-all" (the `-b` flag)
- performance_monitor assumes maximum number of threads are used every time it
  sets performance_monitor::num_threads. Perhaps replace by initializing once
  in init once init routines are working?
  - data generated by `examples/polynomial_expansion/script2*.sh` are not
  completely accurate. My own tests have show that saturation as high as 0.7
  for memory is possible, but these scripts only demonstrate a maximum 
  saturation of 0.4. See the .csv files in 
  `examples/polynomial_expansion/data` for examples.
  - parameters which demonstrated higher saturation are: n=67108864 d=1
    nbiter=800

### Features to add:
- [ ] implement per-core saturation levels
  - [ ] I assume we're not using flops anymore, just 3 key areas above?
  - [ ] how do we benchmark? Just run a single thread? run all threads and
        average? 
  - [ ] change calculate_saturation() so that it calculates per-core and then
        those values are aggregated automatically by
        perform_result_aggregation()
  - [ ] once we decide how we'll do per-core vs. overall saturation, make the
        necessary changes to performance_monitor and saturation_diagram
- [ ] implement new counters that highlight 3 key areas
  - [x] port usage
  - [ ] instruction decoding: can you decode instructions quickly enough?
        (front-end)
  - [ ] micro-instruction retiring: can you fetch instructions quickly
        enough? .... back end? maybe? I'm not sure I understand if "fetching
        instructions" refers to getting new instructions from memory or how the
        back-end sometimes has to wait for operands to be available
  - [ ] TMA is an option for this
- make core saturation (and therefore, color of the core) an average of many
  key metrics.
  - For example, might average flop saturation, instruction decoding, port
    usage, and instruction retiring.
  - This visualization will be general at the initial zoom but when you zoom
    in will separate into the different factors we consider
  - this should include single and double precision flops, because complex
    calculations may use both. However, only consider the highest for the
    average?
  - for more info, see notes taken on what Dr. Saule had to say about the
    subject in DEVELOPMENT_LOG.md -> 2020-06-02 through 2020-06-09 ->
    secondary
- talk to a visualization expert about how we can improve our visualization
- combine benchmark in fhv with benchmark-likwid-vs-manual
  - rewrite computation_measurements to optionally include manual results
  - update CLI to optionally include manual results
- improve benchmark: either decide to use another benchmark or improve the one
  we have
  - consider other benchmark tools (see ["architecture of program"
    section](#architecture-of-program))
  - have it check bandwidth for all types of memory/cache
  - have it check architecture to know what size of caches
  - have it populate architecture.h
- In some cases, color buses instead of components themselves
  - RAM: read/write separately are useful. Also, this is easy to incorporate
  - NUMA: This is the case where it's most important. There's potential for
    the bus(es) between CPUs to be saturated, when it wouldn't be saturated if
    it was memory directly to CPU
- support GOMP_CPU_AFFINITY
- [ ] improve likwid documentation
  - [ ] Tom has not made it clear how to contribute, but here are some ideas
        for PRs:
    - [ ] write some wiki pages about general use (e.g. "There are three ways
          to use likwid...")
    - [ ] write test cases
    - [ ] consider improving doxygen comments and writing man pages for usage
- [ ] consider adding `multimap`s to be used to index the vector of `struct`s
      for easy access
  - what's the use case for this?
- [ ] respect GOMP_CPU_AFFINITY so users can set which specific threads they
      want to use
  - what's the use case for this?
- [ ] create two config files
  - [ ] diagram parameters
  - [ ] architecture (and port_usage and key_metrics and saturation_metrics)
- [ ] make benchmarking long enough to work even on fast clusters

### Improve software engineering
- makefile has some unnecessary repetition of variables
  - compare ./examples/polynomial_expansion/makefile with ./makefile
- [ ] improve software engineering in makefile 
  - [ ] simplify: there's some redundant stuff in there
  - [ ] tests don't work any more
  - [ ] EASY: move convolution rules to own makefile
- EASY: move polynomial expansion into this repository instead of keeping it as a submodule
- [ ] there are a lot of things in tests that simply don't work anymore
  - [ ] verify tests
  - [ ] verify examples
- saturation_diagram.cpp
  - move "num_caches_per_core" to architecture
  - move diagram parameters to config file
- replace `#define`s with `const` declarations
- [ ] there's fhv's csv and performance_monitor's fhv. Should I continue
      to support these moving forward? Are these important tests?
      - if yes, somehow combine them and make the code cleaner
      - if no, remove
- [ ] there are a lot of text files floating around (like in `tests/`). Can
      those be removed?
- [ ] EASY: rename *.h to *.hpp to make it clear they are C++ headers
- [ ] EASY: add fhv_ prefix to fhv constants
- [ ] line 79 of saturation_diagram.cpp: are we just rebuilding list of
      port_usage values there? does it make sense to have a list created in
      performance_monitor_defines.hpp? (the list of port_usage keys *is*
      dynamic so this might be tough)
- [ ] solve port-usage problem
  - [ ] give performance_monitor_defines an init function that can create port
        usage keys dynamically
  - [ ] move key_metrics, saturation, and port_usage keys to architecture
        config file
