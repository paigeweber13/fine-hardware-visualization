# TODO:
## Immediate:
- [ ] double-check legend is representative of how discrete scale works
- [ ] fix IS
  - [x] run diff between my branch and master branch
  - [ ] differences are really very minimal: just enough to get it working on C++
- [ ] Instrument IS
  - [ ] function `rank` does the actual sort
      - weird how it works. Creates threads beforehand with a `#pragma omp
        parallel`, and then does the worksharing within `rank()` using `#pragma
        omp for`
      - they are trying to avoid re-creating threads every iteration
  - [ ] `rank` does some verification but it should be fast
- [ ] Instrument LU
  - [x] `ssor` seems to be the really big one
    - time starts on line 3109
    - time stops on line 3231
  - [x] functions called by ssor: some are expensive, all are important
    - rhs (called by ssor on line 3087)
    - jaclt
    - blts
  - [x] just time all of ssor
    - should be mostly dense linear algebra: so lots of flops and memory runs
    - not sure if likwid allows nested measuring but try to mesure interior functions
    - is pintgr important?
  - [ ] l2norm isn't getting measured for some groups (L2 and L3, notably)
    - [ ] is there a way we can make the visualization message more friendly?
    - [ ] fix this
- [ ] add runtime to JSON and diagram


## Mid-term:
- [ ] fix polynomial_expansion to use new build process
- [ ] make sure convolution pulls header from build dir and not src dir
- [ ] try to get everything inside fhv namespace
- [x] makefile add tab after continued lines
- [x] test now that types and utils have been created
  - [x] test convolution
  - [x] test polynomial expansion
  - had to make some changes to makefile, otherwise fine
- [x] update likwid docs
- [ ] development environment
  - [ ] see if I can get clion or sublime intellisense working
  - [ ] clion works best with cmake... how does sublime work?
  - [x] get desktop set up
- [ ] Setup NAS parallel benchmarks
  - [x] for now, keep as separate project (we don't depend on anything from
        here except built library, anyways). These can be added to examples
        later if we can simplify the code.
  - [x] fork repo, adjust it to use fhv performance monitor
    - [x] convert to compile with g++:
        - [x] replace the variable name `class`, which is a reserved word in 
              c++: `sed -i 's/class/benchmarkClass/g'  **/*.c` or the like. 
              Also, in `wtime.c`, use `gettimeofday(&tv, 0);`
  - [x] start with BT: identify key part and add an fhv region
  - [ ] LU - next priority!
  - [ ] IS - after LU
  - [ ] CG
  - [ ] EP
  - [ ] EP
  - [ ] FT
  - [ ] MG
  - [ ] SP
  - [ ] move `make.def` to `make.def.template`
  - [ ] merge `add-fhv-measuring` into master
- [ ] Using these tests, compare our application to intel vTune
  - [ ] BT
  - [ ] LU
  - [ ] IS
  - [ ] CG
  - [ ] EP
  - [ ] EP
  - [ ] FT
  - [ ] MG
  - [ ] SP
- [ ] conda, conda forge
- [ ] cmake
- [ ] explore how well fhv works with other kernels and codebases
  - [ ] Dr. Saule may be able to throw together some software that
        demonstrates stress on more granular things like TLB or instruction
        decoder
- [ ] start exploring different machines
  - [ ] another skylake architecture with a different number of cores
  - [ ] broadwell/haswell
  - [ ] eventual goal is to have architecture detection totally automatic but
        for now it's adequate to have a few sets of parameters hardcoded that
        are selected automatically

## Long-term:
### Problems to fix:
- if 'FHV_OUTPUT' refers to a folder that does not exist, fhv fails silently.
  Add an error message or create the folder if not exists...
- (If we still want these?) fix benchmark-likwid-vs-manual and 
  thread_migration 
- manual benchmark only prints runtime for flops region
  - in other words, runtime_by_tag doesn't seem to work for more than one 
    region
- sometimes make rule for `run-tests/fhv_minimal` fails with a segmentation
  fault, seems to be right after compilation but before running. Immediately
  running the rule again succeeds.
- sometimes get "stopping non-started region" error in fhv. I think it only
  happens when you run "benchmark-all" (the `-b` flag)
- performance_monitor assumes maximum number of threads are used every time it
  sets performance_monitor::num_threads. Perhaps replace by initializing once
  in init once init routines are working?
  - data generated by `examples/polynomial_expansion/script2*.sh` are not
  completely accurate. My own tests have show that saturation as high as 0.7
  for memory is possible, but these scripts only demonstrate a maximum 
  saturation of 0.4. See the .csv files in 
  `examples/polynomial_expansion/data` for examples.
  - parameters which demonstrated higher saturation are: n=67108864 d=1
    nbiter=800
- cleanup polynomial_expansion:
  - build to separate bin dir
  - adjust scripts to match
- disable implicit type conversion in JSONs. Do this by defining
 `JSON_USE_IMPLICIT_CONVERSIONS` to 0 in the json header.

### Features to add:
- [ ] implement per-core saturation levels
  - [ ] I assume we're not using flops anymore, just 3 key areas above?
  - [ ] how do we benchmark? Just run a single thread? run all threads and
        average? 
  - [ ] change calculate_saturation() so that it calculates per-core and then
        those values are aggregated automatically by
        perform_result_aggregation()
  - [ ] once we decide how we'll do per-core vs. overall saturation, make the
        necessary changes to performance_monitor and saturation_diagram
- [ ] implement new counters that highlight 3 key areas
  - [x] port usage
  - [ ] instruction decoding: can you decode instructions quickly enough?
        (front-end)
  - [ ] micro-instruction retiring: can you fetch instructions quickly
        enough? .... back end? maybe? I'm not sure I understand if "fetching
        instructions" refers to getting new instructions from memory or how the
        back-end sometimes has to wait for operands to be available
  - [ ] TMA is an option for this
- make core saturation (and therefore, color of the core) an average of many
  key metrics.
  - For example, might average flop saturation, instruction decoding, port
    usage, and instruction retiring.
  - This visualization will be general at the initial zoom but when you zoom
    in will separate into the different factors we consider
  - this should include single and double precision flops, because complex
    calculations may use both. However, only consider the highest for the
    average?
  - for more info, see notes taken on what Dr. Saule had to say about the
    subject in DEVELOPMENT_LOG.md -> 2020-06-02 through 2020-06-09 ->
    secondary
- talk to a visualization expert about how we can improve our visualization
- combine benchmark in fhv with benchmark-likwid-vs-manual
  - rewrite computation_measurements to optionally include manual results
  - update CLI to optionally include manual results
- improve benchmark: either decide to use another benchmark or improve the one
  we have
  - consider other benchmark tools (see ["architecture of program"
    section](#architecture-of-program))
  - have it check bandwidth for all types of memory/cache
  - have it check architecture to know what size of caches
  - have it populate architecture.h
- In some cases, color buses instead of components themselves
  - RAM: read/write separately are useful. Also, this is easy to incorporate
  - NUMA: This is the case where it's most important. There's potential for
    the bus(es) between CPUs to be saturated, when it wouldn't be saturated if
    it was memory directly to CPU
- support GOMP_CPU_AFFINITY
- [ ] improve likwid documentation
  - [ ] Tom has not made it clear how to contribute, but here are some ideas
        for PRs:
    - [ ] write some wiki pages about general use (e.g. "There are three ways
          to use likwid...")
    - [ ] write test cases
    - [ ] consider improving doxygen comments and writing man pages for usage
- [ ] consider adding `multimap`s to be used to index the vector of `struct`s
      for easy access
  - what's the use case for this?
- [ ] respect GOMP_CPU_AFFINITY so users can set which specific threads they
      want to use
  - what's the use case for this?
- [ ] create two config files
  - [ ] diagram parameters
  - [ ] architecture (and port_usage and key_metrics and saturation_metrics)
- [ ] make benchmarking long enough to work even on fast clusters

### Improve software engineering
- makefile has some unnecessary repetition of variables
  - compare ./examples/polynomial_expansion/makefile with ./makefile
- [ ] improve software engineering in makefile 
  - [ ] simplify: there's some redundant stuff in there
  - [ ] tests don't work any more
  - [ ] EASY: move convolution rules to own makefile
- EASY: move polynomial expansion into this repository instead of keeping it as a submodule
- [ ] there are a lot of things in tests that simply don't work anymore
  - [ ] verify tests
  - [ ] verify examples
- saturation_diagram.cpp
  - move "num_caches_per_core" to architecture
  - move diagram parameters to config file
- replace `#define`s with `const` declarations
- [ ] there's fhv's csv and performance_monitor's fhv. Should I continue
      to support these moving forward? Are these important tests?
      - if yes, somehow combine them and make the code cleaner
      - if no, remove
- [ ] there are a lot of text files floating around (like in `tests/`). Can
      those be removed?
- [ ] EASY: rename *.h to *.hpp to make it clear they are C++ headers
- [ ] EASY: add fhv_ prefix to fhv constants
- [ ] line 79 of saturation_diagram.cpp: are we just rebuilding list of
      port_usage values there? does it make sense to have a list created in
      performance_monitor_defines.hpp? (the list of port_usage keys *is*
      dynamic so this might be tough)
- [ ] solve port-usage problem
  - [ ] give performance_monitor_defines an init function that can create port
        usage keys dynamically
  - [ ] move key_metrics, saturation, and port_usage keys to architecture
        config file
