# TODO:
## Immediate:
- [ ] re-label current "draw_overview" to "draw_detail" 
- [ ] create new "draw_overview" function
- [ ] add port_usage to diagram
- [ ] quickly finish looking at all perfgroups
- [ ] look at other counters we can use for 3 key areas
  - [ ] port usage
  - [ ] instruction decoding: can you decode instructions quickly enough?
  - [ ] micro-instruction retiring: can you fetch instructions quickly
        enough? 
- [ ] how do we do saturation per-core?
  - [ ] I assume we're not using flops anymore, just 3 key areas above?
  - [ ] how do we benchmark? Just run a single thread? run all threads and
        average? 
  - [ ] change calculate_saturation() so that it calculates per-core and then
        those values are aggregated automatically by
        perform_result_aggregation()
- [x] improve software engineering
  - [x] performance_monitor.cpp (see "improve-result-processing" branch)
    - [x] some things are aggregated across threads, some things are not
    - [x] we're still using groups even though there's really no need. It just
          makes it so we have to iterate through all groups in a region to
          find the right event/metric
    - [x] getting raw data from likwid and aggregating data are tightly
          coupled and difficult to maintain. These should be separated.
- [ ] explore how well fhv works with other kernels and codebases
  - [ ] consider standard benchmarks
  - [ ] Dr. Saule may be able to throw together some software that
        demonstrates stress on more granular things like TLB or instruction
        decoder
- [ ] start exploring different machines
  - [ ] another skylake architecture with a different number of cores
  - [ ] broadwell/haswell
  - [ ] eventual goal is to have architecture detection totally automatic but
        for now it's adequate to have a few sets of parameters hardcoded that
        are selected automatically

## Long-term:
### Problems to fix:
- fix benchmark-likwid-vs-manual and thread_migration 
- manual benchmark only prints runtime for flops region
  - in other words, runtime_by_tag doesn't seem to work for more than one 
    region
- sometimes make rule for `run-tests/fhv_minimal` fails with a segmentation
  fault, seems to be right after compilation but before running. Immediately
  running the rule again succeeds.
- sometimes get "stopping non-started region" error in fhv. I think it only
  happens when you run "benchmark-all" (the `-b` flag)
- performance_monitor assumes maximum number of threads are used every time it
  sets performance_monitor::num_threads. Perhaps replace by initializing once
  in init once init routines are working?
  - data generated by `examples/polynomial_expansion/script2*.sh` are not
  completely accurate. My own tests have show that saturation as high as 0.7
  for memory is possible, but these scripts only demonstrate a maximum 
  saturation of 0.4. See the .csv files in 
  `examples/polynomial_expansion/data` for examples.
  - parameters which demonstrated higher saturation are: n=67108864 d=1
    nbiter=800

### Features to add:
- make core saturation (and therefore, color of the core) an average of many
  key metrics.
  - For example, might average flop saturation, instruction decoding, port
    usage, and instruction retiring.
  - This visualization will be general at the initial zoom but when you zoom
    in will separate into the different factors we consider
  - this should include single and double precision flops, because complex
    calculations may use both. However, only consider the highest for the
    average?
  - for more info, see notes taken on what Dr. Saule had to say about the
    subject in DEVELOPMENT_LOG.md -> 2020-06-02 through 2020-06-09 ->
    secondary
- talk to a visualization expert about how we can improve our visualization
- combine benchmark in fhv with benchmark-likwid-vs-manual
  - rewrite computation_measurements to optionally include manual results
  - update CLI to optionally include manual results
- improve benchmark: either decide to use another benchmark or improve the one
  we have
  - consider other benchmark tools (see ["architecture of program"
    section](#architecture-of-program))
  - have it check bandwidth for all types of memory/cache
  - have it check architecture to know what size of caches
  - have it populate architecture.h
- In some cases, color buses instead of components themselves
  - RAM: read/write separately are useful. Also, this is easy to incorporate
  - NUMA: This is the case where it's most important. There's potential for
    the bus(es) between CPUs to be saturated, when it wouldn't be saturated if
    it was memory directly to CPU
- support GOMP_CPU_AFFINITY
- [ ] improve likwid documentation
  - [ ] Tom has not made it clear how to contribute, but here are some ideas
        for PRs:
    - [ ] write some wiki pages about general use (e.g. "There are three ways
          to use likwid...")
    - [ ] write test cases
    - [ ] consider improving doxygen comments and writing man pages for usage

### Improve software engineering
- makefile has some unnecessary repetition of variables
  - compare ./examples/polynomial_expansion/makefile with ./makefile
- [ ] improve software engineering in makefile 
  - [ ] simplify: there's some redundant stuff in there
  - [ ] tests don't work any more
  - [ ] EASY: move convolution rules to own makefile
- EASY: move polynomial expansion into this repository instead of keeping it as a submodule
- [ ] there are a lot of things in tests that simply don't work anymore
  - [ ] verify tests
  - [ ] verify examples
- saturation_diagram.cpp
  - move "num_caches_per_core" to architecture
  - move diagram parameters to config file
- replace `#define`s with `const` declarations
- [ ] there's fhv's csv and performance_monitor's fhv. Should I continue
      to support these moving forward? Are these important tests?
      - if yes, somehow combine them and make the code cleaner
      - if no, remove
- [ ] there are a lot of text files floating around (like in `tests/`). Can
      those be removed?
